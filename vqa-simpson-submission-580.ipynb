{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.python.keras.saving import hdf5_format\nimport matplotlib.pyplot as plt\nimport json\nimport os\nimport numpy as np\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.utils import plot_model\nimport random\nimport h5py\nfrom sklearn.metrics import classification_report\nimport tensorflow_text as tf_text\n\n##################\n# Verifications:\n#################\nprint('GPU is used.' if len(tf.config.list_physical_devices('GPU')) > 0 else 'GPU is NOT used.')\nprint(\"Tensorflow version: \" + tf.__version__)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-23T13:43:39.736844Z","iopub.execute_input":"2023-06-23T13:43:39.737320Z","iopub.status.idle":"2023-06-23T13:43:49.888675Z","shell.execute_reply.started":"2023-06-23T13:43:39.737294Z","shell.execute_reply":"2023-06-23T13:43:49.887630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Traning Images\ndata_url = \"http://206.12.93.90:8080/simpson2022_dataset/scene_img_abstract_v002_train2015.tar.gz\"\nzip_path = keras.utils.get_file(\"scene_img_abstract_v002_train2015.tar.gz\", data_url, extract=True)\nimgs_path_train = os.path.dirname(zip_path) + '/scene_img_abstract_v002_train2015/'\n\n#  Validation Images\ndata_url = \"http://206.12.93.90:8080/simpson2022_dataset/simpsons_validation.tar.gz\"\nzip_path = keras.utils.get_file(\"simpsons_validation.tar.gz\", data_url, extract=True)\nimgs_path_val = os.path.dirname(zip_path) + '/simpsons_validation/'\n\n# Traning Questions\ndata_url = \"http://206.12.93.90:8080/simpson2022_dataset/OpenEnded_abstract_v002_train2015_questions.zip\"\nzip_path = keras.utils.get_file(\"OpenEnded_abstract_v002_train2015_questions.zip\", data_url,\n                                cache_subdir='datasets/OpenEnded_abstract_v002_train2015_questions/', extract=True)\nq_train_file = os.path.dirname(zip_path) + '/OpenEnded_abstract_v002_train2015_questions.json'\n\n#  Validation Questions\ndata_url = \"http://206.12.93.90:8080/simpson2022_dataset/questions_validation.zip\"\nzip_path = keras.utils.get_file(\"questions_validation.zip\", data_url,\n                                cache_subdir='datasets/questions_validation/', extract=True)\nq_val_file = os.path.dirname(zip_path) + '/questions_validation.json'\n\n# Traning Annotations\ndata_url = \"http://206.12.93.90:8080/simpson2022_dataset/abstract_v002_train2015_annotations.zip\"\nzip_path = keras.utils.get_file(\"abstract_v002_train2015_annotations.zip\", data_url,\n                                cache_subdir='datasets/abstract_v002_train2015_annotations/', extract=True)\nanno_train_file = os.path.dirname(zip_path) + '/abstract_v002_train2015_annotations.json'\n\n# Validation Annotations\ndata_url = \"http://206.12.93.90:8080/simpson2022_dataset/annotations_validation.zip\"\nzip_path = keras.utils.get_file(\"annotations_validation.zip\", data_url,\n                                cache_subdir='datasets/annotations_validation/', extract=True)\nanno_val_file = os.path.dirname(zip_path) + '/annotations_validation.json'","metadata":{"execution":{"iopub.status.busy":"2023-06-23T13:43:49.890832Z","iopub.execute_input":"2023-06-23T13:43:49.891544Z","iopub.status.idle":"2023-06-23T13:46:55.171622Z","shell.execute_reply.started":"2023-06-23T13:43:49.891508Z","shell.execute_reply":"2023-06-23T13:46:55.169673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We define the size of input images to 64x64 pixels.\nimg_width = 256\nimg_height = 256\nimage_size = (img_height, img_width)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-23T13:46:55.174176Z","iopub.execute_input":"2023-06-23T13:46:55.176035Z","iopub.status.idle":"2023-06-23T13:46:55.182026Z","shell.execute_reply.started":"2023-06-23T13:46:55.175995Z","shell.execute_reply":"2023-06-23T13:46:55.180968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answers = ['yes', 'no']\nnum_answers = len(answers)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T13:46:55.184731Z","iopub.execute_input":"2023-06-23T13:46:55.185266Z","iopub.status.idle":"2023-06-23T13:46:55.335094Z","shell.execute_reply.started":"2023-06-23T13:46:55.185233Z","shell.execute_reply":"2023-06-23T13:46:55.334104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the data files\nq_train = json.load(open(q_train_file))\nq_val = json.load(open(q_val_file))\nanno_train = json.load(open(anno_train_file))\nanno_val = json.load(open(anno_val_file))\n\n'''\n    Filter questions for which the answers are not in the set of possible answers.\n'''\n\n\ndef filter_questions(questions, annotations, answers, imgs_path):\n    # Make sure the questions and annotations are alligned\n    questions['questions'] = sorted(questions['questions'], key=lambda x: x['question_id'])\n    annotations['annotations'] = sorted(annotations['annotations'], key=lambda x: x['question_id'])\n    q_out = []\n    anno_out = []\n    imgs_out = []\n    q_ids = []\n    question_ids_set = set()\n    # Filter annotations\n    for annotation in annotations['annotations']:\n        if annotation['multiple_choice_answer'] in answers:\n            question_ids_set.add(annotation['question_id'])\n            q_ids.append(annotation['question_id'])\n            anno_out.append(answers.index(annotation['multiple_choice_answer']))\n    # Filter images and questions\n    i = 0\n    for q in questions['questions']:\n        if q['question_id'] in question_ids_set:\n            # Preprocessing the question\n            q_text = q['question'].lower()\n            q_text = q_text.replace('?', ' ? ')\n            q_text = q_text.replace('.', ' ')\n            q_text = q_text.replace(',', ' ')\n            q_text = q_text.replace('!', ' ').strip()\n            q_out.append(q_text)\n            file_name = str(q['image_id'])\n            while len(file_name) != 12:\n                file_name = '0' + file_name\n            file_name = imgs_path + questions['data_type'] + '_' + questions['data_subtype'] + '_' + file_name + '.png'\n            imgs_out.append(file_name)\n            i = i+1\n    print(i)\n    return imgs_out, q_out, anno_out, q_ids\n\n\nimgs_train, q_train, anno_train, q_ids_train = filter_questions(q_train, anno_train,\n                                                                answers, imgs_path_train)\nimgs_train, q_train, anno_train, q_ids_train = shuffle(imgs_train, q_train,\n                                                       anno_train, q_ids_train, random_state=0)\n\nimgs_val, q_val, anno_val, q_ids_val = filter_questions(q_val, anno_val,\n                                                        answers, imgs_path_val)\nimgs_val, q_val, anno_val, q_ids_val = shuffle(imgs_val, q_val,\n                                               anno_val, q_ids_val, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T13:46:55.336490Z","iopub.execute_input":"2023-06-23T13:46:55.337435Z","iopub.status.idle":"2023-06-23T13:46:57.052861Z","shell.execute_reply.started":"2023-06-23T13:46:55.337402Z","shell.execute_reply":"2023-06-23T13:46:57.051887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nvocab_size = 15000\nvocab = {}\nfor q in q_train:\n    q = word_tokenize(q)\n    for token in q:\n        v = vocab.get(token, 0)\n        vocab[token] = v + 1\nvocab = list(dict(sorted(vocab.items(), key=lambda x: -x[1])[0:vocab_size]).keys())\n#vocab = vocab[:2000]\n# Mapping tokens to integers\ntoken_to_num = keras.layers.StringLookup(vocabulary=vocab)\n# Mapping integers back to original tokens\nnum_to_token = keras.layers.StringLookup(vocabulary=token_to_num.get_vocabulary(),\n                                         invert=True)\nvocab_size = token_to_num.vocabulary_size()\nprint(f\"The size of the vocabulary ={token_to_num.vocabulary_size()}\")\nprint(\"Top 20 tokens in the vocabulary: \", token_to_num.get_vocabulary()[:20])","metadata":{"execution":{"iopub.status.busy":"2023-06-23T13:46:57.054530Z","iopub.execute_input":"2023-06-23T13:46:57.054897Z","iopub.status.idle":"2023-06-23T13:47:04.405508Z","shell.execute_reply.started":"2023-06-23T13:46:57.054865Z","shell.execute_reply":"2023-06-23T13:47:04.404454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n    This function is used to process and encode a single sample.\n'''\ntokenizer = tf_text.UnicodeScriptTokenizer()\n\ndef encode_single_sample(img_file, q, anno):\n    ###########################################\n    ##  Process the Image\n    ##########################################\n    # 1. Read image file  \n    img = tf.io.read_file(img_file)\n    # 2. Decode the image\n    img = tf.image.decode_jpeg(img, channels=3)\n    # 3. Convert to float32 in [0, 1] range\n    img =  tf.image.convert_image_dtype(img, tf.float32)\n    # 4. Resize to the desired size\n    img = tf.image.resize(img, [img_height, img_width])\n    ###########################################\n    ##  Process the question\n    ##########################################\n    # 5. Split into list of tokens\n    word_splits = tokenizer.tokenize(q)\n    # 6. Map tokens to indices\n    q = token_to_num(word_splits)\n    # 7. Return an inputs to for the model\n    return (img, q), anno\n","metadata":{"execution":{"iopub.status.busy":"2023-06-23T13:47:04.406804Z","iopub.execute_input":"2023-06-23T13:47:04.407160Z","iopub.status.idle":"2023-06-23T13:47:04.415016Z","shell.execute_reply.started":"2023-06-23T13:47:04.407128Z","shell.execute_reply":"2023-06-23T13:47:04.413857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We define the batch size\nbatch_size = 128\n# Define the trainig dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n    (imgs_train, q_train, anno_train)\n)\ntrain_dataset = (\n    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n        .padded_batch(batch_size)\n        .prefetch(buffer_size=tf.data.AUTOTUNE)\n)\n# Define the validation dataset\nval_dataset = tf.data.Dataset.from_tensor_slices(\n    (imgs_val, q_val, anno_val)\n)\nval_dataset = (\n    val_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n        .padded_batch(batch_size)\n        .prefetch(buffer_size=tf.data.AUTOTUNE)\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-23T13:47:04.416449Z","iopub.execute_input":"2023-06-23T13:47:04.416997Z","iopub.status.idle":"2023-06-23T13:47:07.438587Z","shell.execute_reply.started":"2023-06-23T13:47:04.416965Z","shell.execute_reply":"2023-06-23T13:47:07.437648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Let's check results on some validation samples\nfor batch in val_dataset.shuffle(random.randint(0, 200)).take(1):\n    imgs_batch = batch[0][0]\n    q_batch = batch[0][1]\n    ans_batch = batch[1]\n    answer_texts = [answers[i] for i in ans_batch]\n\n    q_orig = []\n    for q in q_batch:\n        q = num_to_token(q).numpy()\n        q = [t.decode(\"utf-8\") for t in q]\n        q = list(filter(('[UNK]').__ne__, q))\n        q.insert(int(len(q) / 2), '\\n')\n        q = ' '.join(q)\n        q_orig.append(q)\n\n    _, ax = plt.subplots(4, 4, figsize=(16, 16))\n    for i in range(16):\n        img = (imgs_batch[i, :, :, :] * 255).numpy().astype(np.uint8)\n        title = f\"Q: {q_orig[i]}\\n Ans: {answer_texts[i]}\"\n        ax[i // 4, i % 4].imshow(img)\n        ax[i // 4, i % 4].set_title(title)\n        ax[i // 4, i % 4].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-23T13:47:07.440221Z","iopub.execute_input":"2023-06-23T13:47:07.440596Z","iopub.status.idle":"2023-06-23T13:47:12.398893Z","shell.execute_reply.started":"2023-06-23T13:47:07.440560Z","shell.execute_reply":"2023-06-23T13:47:12.397814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef build_model(img_size, vocab_size, num_answers):\n    # Define the CNN for image input\n    img_input = keras.Input(shape=img_size + (3,), name=\"input_image\")\n    img = layers.Conv2D(16, 3, padding='same')(img_input)\n    img = layers.MaxPooling2D()(img)\n    img = layers.Conv2D(32, 3, padding='same')(img)\n    img = layers.MaxPooling2D()(img)\n    img = layers.Flatten()(img)\n    img = layers.Dense(64, activation='tanh')(img)\n    #Define RNN for language input\n    q_input = keras.Input(shape=(None,), name=\"input_question\")\n    q = layers.Embedding(input_dim=vocab_size, output_dim=20)(q_input)\n    q = layers.SimpleRNN(64)(q)\n    # Combine CNN and RNN\n    mrg = layers.Multiply()([img, q])\n    mrg = layers.Dense(64, activation='tanh')(mrg)\n    # Output    \n    output = layers.Dense(num_answers, activation='softmax', name=\"output\")(mrg)\n    vqa_model = keras.Model(inputs=[img_input, q_input], outputs=output)\n    vqa_model.compile(keras.optimizers.Adam(lr=2e-4),\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n    return vqa_model\n\n\n# Get the model\nmodel = build_model(image_size, vocab_size, num_answers)\nmodel.summary()\nplot_model(model, show_shapes=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-23T15:04:07.518204Z","iopub.execute_input":"2023-06-23T15:04:07.518579Z","iopub.status.idle":"2023-06-23T15:04:07.866035Z","shell.execute_reply.started":"2023-06-23T15:04:07.518547Z","shell.execute_reply":"2023-06-23T15:04:07.865071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    train_dataset,\n    epochs=10,\n    shuffle=True,\n    validation_data=val_dataset\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T15:04:12.630413Z","iopub.execute_input":"2023-06-23T15:04:12.630765Z","iopub.status.idle":"2023-06-23T15:36:10.052357Z","shell.execute_reply.started":"2023-06-23T15:04:12.630735Z","shell.execute_reply":"2023-06-23T15:36:10.051318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('model.h5', overwrite=True, save_format='h5')\nwith h5py.File('model.h5', mode='a') as f:\n    f.create_dataset('answers', data=answers)\n    f.create_dataset('vocab', data=vocab)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-23T15:57:11.836364Z","iopub.execute_input":"2023-06-23T15:57:11.837077Z","iopub.status.idle":"2023-06-23T15:57:12.149328Z","shell.execute_reply.started":"2023-06-23T15:57:11.837042Z","shell.execute_reply":"2023-06-23T15:57:12.148389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def learning_plots(history):\n    plt.figure(figsize=(15, 4))\n    ax1 = plt.subplot(1, 2, 1)\n    for l in history.history:\n        if l == 'loss' or l == 'val_loss':\n            loss = history.history[l]\n            plt.plot(range(1, len(loss) + 1), loss, label=l)\n\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    ax2 = plt.subplot(1, 2, 2)\n    for k in history.history:\n        if 'accuracy' in k:\n            loss = history.history[k]\n            plt.plot(range(1, len(loss) + 1), loss, label=k)\n    plt.title('Training and validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n\n\nlearning_plots(history)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-23T15:57:17.392381Z","iopub.execute_input":"2023-06-23T15:57:17.393022Z","iopub.status.idle":"2023-06-23T15:57:17.883445Z","shell.execute_reply.started":"2023-06-23T15:57:17.392991Z","shell.execute_reply":"2023-06-23T15:57:17.882401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\nmodel = load_model('model.h5')\ny_prob = model.predict(val_dataset)\ny_pred = y_prob.argmax(axis=-1)\nprint(classification_report(anno_val, y_pred, target_names=answers))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-23T15:57:28.099539Z","iopub.execute_input":"2023-06-23T15:57:28.099895Z","iopub.status.idle":"2023-06-23T15:57:31.212612Z","shell.execute_reply.started":"2023-06-23T15:57:28.099866Z","shell.execute_reply":"2023-06-23T15:57:31.211465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in val_dataset.shuffle(20).take(1):\n    imgs_batch = batch[0][0]\n    q_batch = batch[0][1]\n    ans_batch = batch[1]    \n    answer_texts = [answers[i] for i in ans_batch]\n    preds = model.predict(batch[0])\n    pred_texts = [answers[i] for i in np.argmax(preds, axis=1)]\n    q_orig = []\n    for q in q_batch:\n        q = num_to_token(q).numpy()\n        q = [t.decode(\"utf-8\") for t in q]\n        q = list(filter(('[UNK]').__ne__, q))\n        q.insert(int(len(q) / 2), '\\n')\n        q = ' '.join(q)\n        q_orig.append(q)\n    _, ax = plt.subplots(2, 4, figsize=(16, 16))\n    for i in range(8):\n        example_input = (np.array([imgs_batch[i]]), np.array([q_batch[i]]))\n        title = f\"Q: {q_orig[i]}\\n Pred Ans: {pred_texts[i]} \\n True Ans: {answer_texts[i]}\"\n        ax[i // 4, i % 4].imshow(imgs_batch[i])\n        ax[i // 4, i % 4].set_title(title)\n        ax[i // 4, i % 4].axis(\"off\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-23T15:57:39.177874Z","iopub.execute_input":"2023-06-23T15:57:39.178226Z","iopub.status.idle":"2023-06-23T15:57:43.453006Z","shell.execute_reply.started":"2023-06-23T15:57:39.178196Z","shell.execute_reply":"2023-06-23T15:57:43.452150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}